{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Akash Sharma\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Akash Sharma\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Akash Sharma\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Akash Sharma\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Akash Sharma\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Akash Sharma\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Akash Sharma\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Akash Sharma\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Akash Sharma\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Akash Sharma\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Akash Sharma\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Akash Sharma\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(490, 18528) (236, 18528)\n",
      "(14822, 490) (3706, 490)\n",
      "WARNING:tensorflow:From C:\\Users\\Akash Sharma\\Desktop\\Final Year Project\\PSSE-via-DNNs-master\\model.py:20: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\Akash Sharma\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/200\n",
      "14822/14822 [==============================] - 31s 2ms/step - loss: 7.1444 - mae: 7.6256\n",
      "Epoch 2/200\n",
      "14822/14822 [==============================] - 20s 1ms/step - loss: 4.7032 - mae: 5.1796\n",
      "Epoch 3/200\n",
      "14822/14822 [==============================] - 21s 1ms/step - loss: 4.0583 - mae: 4.5343\n",
      "Epoch 4/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 3.6628 - mae: 4.1415\n",
      "Epoch 5/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 2.4749 - mae: 2.9418\n",
      "Epoch 6/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 2.0369 - mae: 2.4944\n",
      "Epoch 7/200\n",
      "14822/14822 [==============================] - 16s 1ms/step - loss: 2.2636 - mae: 2.7199\n",
      "Epoch 8/200\n",
      "14822/14822 [==============================] - 16s 1ms/step - loss: 2.1002 - mae: 2.5631: 0s - loss: 2.0931 - mae: 2.\n",
      "Epoch 9/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 2.2810 - mae: 2.7392\n",
      "Epoch 10/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 2.4714 - mae: 2.9320: 0s - loss: 2.4081 - mae: 2\n",
      "Epoch 11/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 1.9656 - mae: 2.4167\n",
      "Epoch 12/200\n",
      "14822/14822 [==============================] - 16s 1ms/step - loss: 1.6859 - mae: 2.1447\n",
      "Epoch 13/200\n",
      "14822/14822 [==============================] - 16s 1ms/step - loss: 1.5807 - mae: 2.0234\n",
      "Epoch 14/200\n",
      "14822/14822 [==============================] - 16s 1ms/step - loss: 1.0158 - mae: 1.4284\n",
      "Epoch 15/200\n",
      "14822/14822 [==============================] - 16s 1ms/step - loss: 1.9976 - mae: 2.4504\n",
      "Epoch 16/200\n",
      "14822/14822 [==============================] - 16s 1ms/step - loss: 1.4018 - mae: 1.8463\n",
      "Epoch 17/200\n",
      "14822/14822 [==============================] - 16s 1ms/step - loss: 1.2522 - mae: 1.6838\n",
      "Epoch 18/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 1.3542 - mae: 1.7912\n",
      "Epoch 19/200\n",
      "14822/14822 [==============================] - 16s 1ms/step - loss: 1.8192 - mae: 2.2664\n",
      "Epoch 20/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 1.4635 - mae: 1.9205\n",
      "Epoch 21/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 1.5142 - mae: 1.9710\n",
      "Epoch 22/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 1.1135 - mae: 1.5473\n",
      "Epoch 23/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.4319 - mae: 0.8171\n",
      "Epoch 24/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.6466 - mae: 1.0405\n",
      "Epoch 25/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.8544 - mae: 1.2703: 2s - loss: 0.6940 - mae: 1 - E\n",
      "Epoch 26/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.9708 - mae: 1.3855\n",
      "Epoch 27/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.4967 - mae: 0.8857\n",
      "Epoch 28/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 1.2828 - mae: 1.7296\n",
      "Epoch 29/200\n",
      "14822/14822 [==============================] - 19s 1ms/step - loss: 0.6299 - mae: 1.0179\n",
      "Epoch 30/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.9768 - mae: 1.3998\n",
      "Epoch 31/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.4465 - mae: 0.8342\n",
      "Epoch 32/200\n",
      "14822/14822 [==============================] - 16s 1ms/step - loss: 0.4716 - mae: 0.8610\n",
      "Epoch 33/200\n",
      "14822/14822 [==============================] - 19s 1ms/step - loss: 0.5133 - mae: 0.9115\n",
      "Epoch 34/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.9117 - mae: 1.3247\n",
      "Epoch 35/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.4578 - mae: 0.8478\n",
      "Epoch 36/200\n",
      "14822/14822 [==============================] - 16s 1ms/step - loss: 0.6160 - mae: 1.0134\n",
      "Epoch 37/200\n",
      "14822/14822 [==============================] - 16s 1ms/step - loss: 0.5563 - mae: 0.9555\n",
      "Epoch 38/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.9736 - mae: 1.3934\n",
      "Epoch 39/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.4379 - mae: 0.8297\n",
      "Epoch 40/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.7832 - mae: 1.1866\n",
      "Epoch 41/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.8584 - mae: 1.2689\n",
      "Epoch 42/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.6543 - mae: 1.0421\n",
      "Epoch 43/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3782 - mae: 0.7451\n",
      "Epoch 44/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.4090 - mae: 0.7937\n",
      "Epoch 45/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.8396 - mae: 1.2422\n",
      "Epoch 46/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 1.1023 - mae: 1.5337\n",
      "Epoch 47/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3345 - mae: 0.6842\n",
      "Epoch 48/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.3884 - mae: 0.7514\n",
      "Epoch 49/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.6644 - mae: 1.0508: 0s - loss: 0.6655 - mae: 1.051\n",
      "Epoch 50/200\n",
      "14822/14822 [==============================] - 16s 1ms/step - loss: 0.5782 - mae: 0.9620:\n",
      "Epoch 51/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.4083 - mae: 0.7852: 0s - loss: 0.4095 \n",
      "Epoch 52/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3999 - mae: 0.7761\n",
      "Epoch 53/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.9711 - mae: 1.3849\n",
      "Epoch 54/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.4221 - mae: 0.7959\n",
      "Epoch 55/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3894 - mae: 0.7604\n",
      "Epoch 56/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 1.2199 - mae: 1.6751\n",
      "Epoch 57/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3924 - mae: 0.6940\n",
      "Epoch 58/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3420 - mae: 0.6851\n",
      "Epoch 59/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.4025 - mae: 0.7880\n",
      "Epoch 60/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.5167 - mae: 0.8936\n",
      "Epoch 61/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3583 - mae: 0.7141\n",
      "Epoch 62/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 1.0896 - mae: 1.5284\n",
      "Epoch 63/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 1.1636 - mae: 1.6258\n",
      "Epoch 64/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 1.0177 - mae: 1.4759\n",
      "Epoch 65/200\n",
      "14822/14822 [==============================] - 19s 1ms/step - loss: 0.4007 - mae: 0.7033\n",
      "Epoch 66/200\n",
      "14822/14822 [==============================] - 19s 1ms/step - loss: 0.2152 - mae: 0.4417\n",
      "Epoch 67/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.2956 - mae: 0.5997\n",
      "Epoch 68/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.3463 - mae: 0.6944\n",
      "Epoch 69/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.6015 - mae: 0.9810\n",
      "Epoch 70/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3625 - mae: 0.7242: 7s - loss: 0.3513 - ma - ETA: 6s - loss: 0.3499\n",
      "Epoch 71/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3661 - mae: 0.7318\n",
      "Epoch 72/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3487 - mae: 0.7001\n",
      "Epoch 73/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3943 - mae: 0.7574: 1s - loss: 0.38\n",
      "Epoch 74/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 1.1743 - mae: 1.6192\n",
      "Epoch 75/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 1.1036 - mae: 1.5608\n",
      "Epoch 76/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.2363 - mae: 0.4792\n",
      "Epoch 77/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.2398 - mae: 0.5041\n",
      "Epoch 78/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3429 - mae: 0.6999\n",
      "Epoch 79/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3489 - mae: 0.7044\n",
      "Epoch 80/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.4650 - mae: 0.8459\n",
      "Epoch 81/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 1.2273 - mae: 1.6764\n",
      "Epoch 82/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.4260 - mae: 0.7214: 4s - loss: 0.4\n",
      "Epoch 83/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.2391 - mae: 0.5145\n",
      "Epoch 84/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3283 - mae: 0.6659\n",
      "Epoch 85/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3876 - mae: 0.7502\n",
      "Epoch 86/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3761 - mae: 0.7410\n",
      "Epoch 87/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3177 - mae: 0.6494\n",
      "Epoch 88/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3449 - mae: 0.6971: 2\n",
      "Epoch 89/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3474 - mae: 0.6982\n",
      "Epoch 90/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3547 - mae: 0.7203\n",
      "Epoch 91/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.3514 - mae: 0.7055\n",
      "Epoch 92/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.3418 - mae: 0.6962\n",
      "Epoch 93/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3506 - mae: 0.7045\n",
      "Epoch 94/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3367 - mae: 0.6838\n",
      "Epoch 95/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3518 - mae: 0.7050\n",
      "Epoch 96/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3301 - mae: 0.6736\n",
      "Epoch 97/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3278 - mae: 0.6711\n",
      "Epoch 98/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3587 - mae: 0.7243\n",
      "Epoch 99/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3307 - mae: 0.6761\n",
      "Epoch 100/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.3257 - mae: 0.6710\n",
      "Epoch 101/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3363 - mae: 0.6792\n",
      "Epoch 102/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3374 - mae: 0.6782\n",
      "Epoch 103/200\n",
      "14822/14822 [==============================] - 22s 1ms/step - loss: 0.3507 - mae: 0.6972\n",
      "Epoch 104/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3292 - mae: 0.6720\n",
      "Epoch 105/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.6225 - mae: 0.9966\n",
      "Epoch 106/200\n",
      "14822/14822 [==============================] - 19s 1ms/step - loss: 0.3718 - mae: 0.7197\n",
      "Epoch 107/200\n",
      "14822/14822 [==============================] - 19s 1ms/step - loss: 0.3144 - mae: 0.6397\n",
      "Epoch 108/200\n",
      "14822/14822 [==============================] - 20s 1ms/step - loss: 0.3213 - mae: 0.6606\n",
      "Epoch 109/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.3352 - mae: 0.6856\n",
      "Epoch 110/200\n",
      "14822/14822 [==============================] - 19s 1ms/step - loss: 0.3183 - mae: 0.6477\n",
      "Epoch 111/200\n",
      "14822/14822 [==============================] - 19s 1ms/step - loss: 0.3233 - mae: 0.6659\n",
      "Epoch 112/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.3470 - mae: 0.6964\n",
      "Epoch 113/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3126 - mae: 0.6437\n",
      "Epoch 114/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.3296 - mae: 0.6743\n",
      "Epoch 115/200\n",
      "14822/14822 [==============================] - 19s 1ms/step - loss: 0.3240 - mae: 0.6559\n",
      "Epoch 116/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.3328 - mae: 0.6781\n",
      "Epoch 117/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.3224 - mae: 0.6629\n",
      "Epoch 118/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.3279 - mae: 0.6734\n",
      "Epoch 119/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.3260 - mae: 0.6669: 1s - loss:\n",
      "Epoch 120/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.3127 - mae: 0.6535\n",
      "Epoch 121/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.3327 - mae: 0.6754\n",
      "Epoch 122/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.3404 - mae: 0.6837\n",
      "Epoch 123/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.3192 - mae: 0.6594\n",
      "Epoch 124/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.3302 - mae: 0.6685\n",
      "Epoch 125/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.3180 - mae: 0.6531\n",
      "Epoch 126/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.3221 - mae: 0.6591\n",
      "Epoch 127/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.3249 - mae: 0.6647\n",
      "Epoch 128/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.3195 - mae: 0.6558\n",
      "Epoch 129/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.3102 - mae: 0.6378\n",
      "Epoch 130/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3374 - mae: 0.6757\n",
      "Epoch 131/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.5352 - mae: 0.8922\n",
      "Epoch 132/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.3180 - mae: 0.6457\n",
      "Epoch 133/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.3225 - mae: 0.6579\n",
      "Epoch 134/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.3040 - mae: 0.6416: 2s  - ETA: 0s - loss: 0.3044 - mae: 0.64\n",
      "Epoch 135/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.3121 - mae: 0.6432\n",
      "Epoch 136/200\n",
      "14822/14822 [==============================] - 19s 1ms/step - loss: 0.3102 - mae: 0.6395\n",
      "Epoch 137/200\n",
      "14822/14822 [==============================] - 19s 1ms/step - loss: 0.3183 - mae: 0.6526\n",
      "Epoch 138/200\n",
      "14822/14822 [==============================] - 19s 1ms/step - loss: 0.3159 - mae: 0.6526\n",
      "Epoch 139/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.3184 - mae: 0.6581\n",
      "Epoch 140/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.3136 - mae: 0.6444\n",
      "Epoch 141/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.3323 - mae: 0.6745\n",
      "Epoch 142/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.3049 - mae: 0.6292\n",
      "Epoch 143/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.3171 - mae: 0.6520\n",
      "Epoch 144/200\n",
      "14822/14822 [==============================] - 19s 1ms/step - loss: 0.3164 - mae: 0.6563\n",
      "Epoch 145/200\n",
      "14822/14822 [==============================] - 20s 1ms/step - loss: 0.3086 - mae: 0.6424\n",
      "Epoch 146/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3147 - mae: 0.6525\n",
      "Epoch 147/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.3076 - mae: 0.6304\n",
      "Epoch 148/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3058 - mae: 0.6373\n",
      "Epoch 157/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3033 - mae: 0.6330: 3s - l - ETA: 1s -\n",
      "Epoch 158/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3070 - mae: 0.6357\n",
      "Epoch 159/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.3174 - mae: 0.6542\n",
      "Epoch 160/200\n",
      "14822/14822 [==============================] - 17s 1ms/step - loss: 0.2928 - mae: 0.6134\n",
      "Epoch 177/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.2941 - mae: 0.6219\n",
      "Epoch 178/200\n",
      "14822/14822 [==============================] - 21s 1ms/step - loss: 0.3075 - mae: 0.6381\n",
      "Epoch 179/200\n",
      "14822/14822 [==============================] - 20s 1ms/step - loss: 0.2944 - mae: 0.6176\n",
      "Epoch 180/200\n",
      "14822/14822 [==============================] - 21s 1ms/step - loss: 0.2977 - mae: 0.6233\n",
      "Epoch 181/200\n",
      "14822/14822 [==============================] - 22s 2ms/step - loss: 0.2950 - mae: 0.6047: 2s - loss\n",
      "Epoch 182/200\n",
      "14822/14822 [==============================] - 20s 1ms/step - loss: 0.2982 - mae: 0.6236\n",
      "Epoch 183/200\n",
      "14822/14822 [==============================] - 20s 1ms/step - loss: 0.2993 - mae: 0.6302\n",
      "Epoch 184/200\n",
      "14822/14822 [==============================] - 19s 1ms/step - loss: 0.2910 - mae: 0.6079: 0s - loss: 0.2908 - mae: 0\n",
      "Epoch 185/200\n",
      "14822/14822 [==============================] - 19s 1ms/step - loss: 0.3022 - mae: 0.6340\n",
      "Epoch 186/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.2976 - mae: 0.6162\n",
      "Epoch 187/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.2863 - mae: 0.5966\n",
      "Epoch 188/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.2894 - mae: 0.6071\n",
      "Epoch 189/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.3008 - mae: 0.6261\n",
      "Epoch 190/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.2919 - mae: 0.6081: 1s - loss: 0.2948\n",
      "Epoch 191/200\n",
      "14822/14822 [==============================] - 19s 1ms/step - loss: 0.2979 - mae: 0.6140\n",
      "Epoch 192/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.2888 - mae: 0.6019: 0s - loss: 0.2909 - \n",
      "Epoch 193/200\n",
      "14822/14822 [==============================] - 19s 1ms/step - loss: 0.2858 - mae: 0.6007\n",
      "Epoch 194/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.2970 - mae: 0.6271\n",
      "Epoch 195/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.3077 - mae: 0.6406\n",
      "Epoch 196/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.2921 - mae: 0.6123\n",
      "Epoch 197/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.2787 - mae: 0.5856\n",
      "Epoch 198/200\n",
      "14822/14822 [==============================] - 19s 1ms/step - loss: 0.2886 - mae: 0.6058\n",
      "Epoch 199/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.2965 - mae: 0.6276\n",
      "Epoch 200/200\n",
      "14822/14822 [==============================] - 18s 1ms/step - loss: 0.2818 - mae: 0.5962\n",
      "\n",
      "Saving model weights to model_logs\\118_nn1_8H_PSSE_epoch_200.h5\n",
      "3706/3706 [==============================] - 2s 492us/step\n",
      "\n",
      "mae: 85.34%\n",
      "3706\n",
      "\n",
      " distance from the true states in terms of \\|\\|_2: 0.2350%\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "SEED=1234\n",
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "np.random.seed(SEED)\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import os, shutil, scipy.io\n",
    "from model import *\n",
    "\n",
    "# configure args\n",
    "tf.set_random_seed(SEED)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "K.set_learning_phase(1)\n",
    "\n",
    "# data loading part\n",
    "caseNo = 118\n",
    "weight_4_mag = 100\n",
    "weight_4_ang = 1#2*math.pi/360\n",
    "\n",
    "psse_data = scipy.io.loadmat('dist2_118FASE_data.mat')\n",
    "print(psse_data['inputs'].shape, psse_data['labels'].shape)\n",
    "\n",
    "data_x = psse_data['inputs']\n",
    "data_y = psse_data['labels']\n",
    "\n",
    "# scale the mags,\n",
    "data_y[0:caseNo,:] = weight_4_mag*data_y[0:caseNo,:]\n",
    "data_y[caseNo:,:] = weight_4_ang*data_y[caseNo:,:]\n",
    "\n",
    "\n",
    "# seperate them into training 80%, test 20%\n",
    "split_train = int(0.8*psse_data['inputs'].shape[1])\n",
    "split_val = psse_data['inputs'].shape[1] - split_train #int(0.25*psse_data['inputs'].shape[1])\n",
    "train_x = np.transpose(data_x[:, :split_train])\n",
    "train_y = np.transpose(data_y[:, :split_train])\n",
    "val_x   = np.transpose(data_x[:, split_train:split_train+split_val])\n",
    "val_y   = np.transpose(data_y[:, split_train:split_train+split_val])\n",
    "test_x  = np.transpose(data_x[:, split_train+split_val:])\n",
    "test_y  = np.transpose(data_y[:, split_train+split_val:])\n",
    "\n",
    "print(train_x.shape, val_x.shape)\n",
    "#Train the model\n",
    "input_shape = (train_x.shape[1],)\n",
    "\n",
    "epoch_num = 200\n",
    "psse_model = nn1_8H_psse(input_shape, train_y.shape[1])\n",
    "psse_model.fit(train_x, train_y, epochs=epoch_num, batch_size=64)\n",
    "\n",
    "save_file = '_'.join([str(caseNo), 'nn1_8H_PSSE',\n",
    "                      'epoch', str(epoch_num)]) + '.h5'\n",
    "\n",
    "if not os.path.exists('model_logs'):\n",
    "    os.makedirs('model_logs')\n",
    "save_path = os.path.join('model_logs', save_file)\n",
    "print('\\nSaving model weights to {:s}'.format(save_path))\n",
    "psse_model.save_weights(save_path)\n",
    "\n",
    "\n",
    "# evaluate the model\n",
    "K.set_learning_phase(0)\n",
    "val_predic = psse_model.predict(val_x)\n",
    "scores = psse_model.evaluate(val_x, val_y)\n",
    "print(\"\\n%s: %.2f%%\" % (psse_model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "\n",
    "#the self.defined distance metric since, to access the distance between predicted and the true\n",
    "print(val_y.shape[0])\n",
    "test_no = 3706\n",
    "def rmse(val_predic, val_y, voltage_distance = np.zeros((test_no,caseNo)), voltage_norm = np.zeros((test_no,1))):\n",
    "    for i in range(test_no):\n",
    "        for j in range(caseNo):\n",
    "            predic_r, predic_i = (1/weight_4_mag)* val_predic[i, j]*math.cos(val_predic[i, j+caseNo]*2*math.pi/360), (1/weight_4_mag)*val_predic[i,j]*math.sin(val_predic[i, j+caseNo]*2*math.pi/360)\n",
    "            val_r, val_i = (1/weight_4_mag)*val_y[i,j]*math.cos(val_y[i,j+caseNo]*2*math.pi/360), (1/weight_4_mag)*val_y[i][j]*math.sin(val_y[i][j+caseNo]*2*math.pi/360)\n",
    "            voltage_distance[i,j] = (predic_r-val_r)**2 + (predic_i-val_i)**2\n",
    "            #print(i, j, val_predic[i, j], val_predic[i, j+caseNo], val_y[i,j], val_y[i,j+caseNo])\n",
    "        voltage_norm[i,] = (1/caseNo)*np.sqrt(np.sum(voltage_distance[i,:]))\n",
    "    return np.mean(voltage_norm) *100\n",
    "print(\"\\n distance from the true states in terms of \\|\\|_2: %.4f%%\" % rmse(val_predic, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
